{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install huggingface_hub transformers Langchain"
      ],
      "metadata": {
        "id": "0uKvCbBI_5BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vers_WgY_pHg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "# Initialize sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Initialize sentiment counters\n",
        "sentiment_counters = Counter()\n",
        "\n",
        "print(\"Welcome to the GPT-3 Conversation! Type 'exit' to quit.\")\n",
        "\n",
        "chat_history_ids = None\n",
        "\n",
        "while True:\n",
        "    new_user_input = input(\">> User: \")\n",
        "    if new_user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    new_user_input_ids = tokenizer.encode(new_user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
        "\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    generated_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    # Analyze sentiment of the generated response\n",
        "    sentiment_result = sentiment_pipeline(generated_response)\n",
        "\n",
        "    # Update sentiment counters\n",
        "    sentiment_counters.update([sentiment_result[0]['label']])\n",
        "\n",
        "    # Print the generated response and its sentiment\n",
        "    print(f\"DialoGPT: {generated_response}\")\n",
        "    print(f\"Sentiment: {sentiment_result[0]['label']} (Score: {sentiment_result[0]['score']})\")\n",
        "\n",
        "# Generate sentiment analysis report\n",
        "print(\"\\n===== Sentiment Analysis Report =====\")\n",
        "total_responses = sum(sentiment_counters.values())\n",
        "print(f\"Total responses: {total_responses}\")\n",
        "\n",
        "for sentiment, count in sentiment_counters.items():\n",
        "    percentage = (count / total_responses) * 100\n",
        "    print(f\"{sentiment.capitalize()} sentiment: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "# Generate pie chart visualization\n",
        "labels = sentiment_counters.keys()\n",
        "sizes = sentiment_counters.values()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "ax.axis('equal')\n",
        "ax.set_title('Sentiment Distribution')\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ]
}